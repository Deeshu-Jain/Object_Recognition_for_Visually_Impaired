{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using YOLO\n",
    "YOLO(You Only Look Once) is an object detector that uses features learned by a deep convolutional neural network to detect an object. \n",
    "\n",
    "YOLO trained on the COCO dataset.The dataset consists of 80 labels, including, but not limited to:\n",
    "People, <BR>\n",
    "Bicycles, <BR>\n",
    "Cars and trucks, <BR>\n",
    "Airplanes, <BR>\n",
    "Stop signs and fire hydrants, <BR>\n",
    "Animals, including cats, dogs, birds, horses, cows, and sheep, and <BR>\n",
    "Kitchen and dining objects, such as wine glasses, cups, forks, knives, spoons, etc. â€¦and much more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 #load our images\n",
    "import matplotlib.pyplot as plt #plot them,\n",
    "from utils import *  #module that contains some helper functions\n",
    "from darknet import Darknet #modified version of *Darknet*. YOLO uses *Darknet*,an open source, deep neural network framework written by the creators of YOLO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading weights. Please Wait...100.00% Complete\r"
     ]
    }
   ],
   "source": [
    "# Set the location and name of the cfg file that contains the network architecture\n",
    "cfg_file = './cfg/yolov3.cfg'\n",
    "\n",
    "# Set the location and name of the pre-trained weights file that contains the pre-trained weights\n",
    "weight_file = './weights/yolov3.weights'\n",
    "\n",
    "# Set the location and name of COCO object classes file that has the list of the 80 object classes that the weights were trained to detect.\n",
    "namesfile = './data/coco.names'\n",
    "\n",
    "# Load the network architecture\n",
    "m = Darknet(cfg_file)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "m.load_weights(weight_file)\n",
    "\n",
    "# Load the COCO object classes\n",
    "class_names = load_class_names(namesfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking a Look at The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer     filters    size              input                output\n",
      "    0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32\n",
      "    1 conv     64  3 x 3 / 2   416 x 416 x  32   ->   208 x 208 x  64\n",
      "    2 conv     32  1 x 1 / 1   208 x 208 x  64   ->   208 x 208 x  32\n",
      "    3 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64\n",
      "    4 shortcut 1\n",
      "    5 conv    128  3 x 3 / 2   208 x 208 x  64   ->   104 x 104 x 128\n",
      "    6 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64\n",
      "    7 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "    8 shortcut 5\n",
      "    9 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64\n",
      "   10 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "   11 shortcut 8\n",
      "   12 conv    256  3 x 3 / 2   104 x 104 x 128   ->    52 x  52 x 256\n",
      "   13 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   14 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   15 shortcut 12\n",
      "   16 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   17 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   18 shortcut 15\n",
      "   19 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   20 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   21 shortcut 18\n",
      "   22 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   23 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   24 shortcut 21\n",
      "   25 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   26 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   27 shortcut 24\n",
      "   28 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   29 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   30 shortcut 27\n",
      "   31 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   32 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   33 shortcut 30\n",
      "   34 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   35 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   36 shortcut 33\n",
      "   37 conv    512  3 x 3 / 2    52 x  52 x 256   ->    26 x  26 x 512\n",
      "   38 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   39 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   40 shortcut 37\n",
      "   41 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   42 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   43 shortcut 40\n",
      "   44 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   45 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   46 shortcut 43\n",
      "   47 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   48 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   49 shortcut 46\n",
      "   50 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   51 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   52 shortcut 49\n",
      "   53 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   54 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   55 shortcut 52\n",
      "   56 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   57 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   58 shortcut 55\n",
      "   59 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   60 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   61 shortcut 58\n",
      "   62 conv   1024  3 x 3 / 2    26 x  26 x 512   ->    13 x  13 x1024\n",
      "   63 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   64 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   65 shortcut 62\n",
      "   66 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   67 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   68 shortcut 65\n",
      "   69 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   70 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   71 shortcut 68\n",
      "   72 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   73 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   74 shortcut 71\n",
      "   75 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   76 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   77 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   78 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   79 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   80 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   81 conv    255  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 255\n",
      "   82 detection\n",
      "   83 route  79\n",
      "   84 conv    256  1 x 1 / 1    13 x  13 x 512   ->    13 x  13 x 256\n",
      "   85 upsample           * 2    13 x  13 x 256   ->    26 x  26 x 256\n",
      "   86 route  85 61\n",
      "   87 conv    256  1 x 1 / 1    26 x  26 x 768   ->    26 x  26 x 256\n",
      "   88 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   89 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   90 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   91 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   92 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   93 conv    255  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 255\n",
      "   94 detection\n",
      "   95 route  91\n",
      "   96 conv    128  1 x 1 / 1    26 x  26 x 256   ->    26 x  26 x 128\n",
      "   97 upsample           * 2    26 x  26 x 128   ->    52 x  52 x 128\n",
      "   98 route  97 36\n",
      "   99 conv    128  1 x 1 / 1    52 x  52 x 384   ->    52 x  52 x 128\n",
      "  100 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "  101 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "  102 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "  103 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "  104 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "  105 conv    255  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 255\n",
      "  106 detection\n"
     ]
    }
   ],
   "source": [
    "# Print the neural network used in YOLOv3\n",
    "m.print_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the neural network used by YOLOv3 consists mainly of convolutional layers, with some shortcut connections and upsample layers. For a full description of this network please refer to the <a href=\"https://pjreddie.com/media/files/papers/YOLOv3.pdf\">YOLOv3 Paper</a>.\n",
    "\n",
    "# Loading and Resizing Our Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# load our images using OpenCV's cv2.imread() function.\n",
    "img = cv2.imread('./images/city_scene.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Since, this function loads images as BGR we will convert our images to RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# We resize the image to the input width and height of the first layer of the network ie 416 x 416 x 3.    \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERNEL DYING (WHEN TRYING TO SHOW IMAGES....)\n",
    "### CODE HERE\n",
    "### Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Resized Image')\n",
    "plt.imshow(resized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Non-Maximal Suppression (NMS) Threshold\n",
    " YOLO uses **Non-Maximal Suppression (NMS)** to only keep the best bounding box. The first step in NMS is to remove all the predicted bounding boxes that have a detection probability that is less than a given NMS threshold.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the NMS threshold\n",
    "nms_thresh = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Intersection Over Union (IOU) Threshold\n",
    "\n",
    "After removing all the predicted bounding boxes that have a low detection probability, the second step in NMS, is to select the bounding boxes with the highest detection probability and eliminate all the bounding boxes whose **Intersection Over Union (IOU)** value is higher than a given IOU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the IOU threshold\n",
    "iou_thresh = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It took 0.635 seconds to detect the objects in the image.\n",
      "\n",
      "Number of Objects Detected: 17 \n",
      "\n",
      "Objects Found and Confidence Level:\n",
      "\n",
      "1. person: 0.999996\n",
      "2. person: 1.000000\n",
      "3. car: 0.707238\n",
      "4. truck: 0.933031\n",
      "5. car: 0.658085\n",
      "6. truck: 0.666981\n",
      "7. person: 1.000000\n",
      "8. traffic light: 1.000000\n",
      "9. person: 1.000000\n",
      "10. car: 0.997369\n",
      "11. bus: 0.998023\n",
      "12. person: 1.000000\n",
      "13. person: 1.000000\n",
      "14. person: 1.000000\n",
      "15. person: 1.000000\n",
      "16. person: 0.999990\n",
      "17. person: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Detect objects in the image\n",
    "boxes = detect_objects(m, resized_image, iou_thresh, nms_thresh)\n",
    "\n",
    "# Print the objects found and the confidence level\n",
    "print_objects(boxes, class_names)\n",
    "\n",
    "#Plot the image with bounding boxes and corresponding object class labels\n",
    "plot_boxes(original_image, boxes, class_names, plot_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
